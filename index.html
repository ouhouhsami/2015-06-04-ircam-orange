<!DOCTYPE html>
<html>
  <head>
    <title>Web Audio & Applications</title>
    <meta charset="utf-8">
    <style>
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

      body { font-family: 'Droid Serif'; }
      h1, h2, h3 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: normal;
      }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
    </style>
  </head>
  <body>
    <textarea id="source">

class: center, middle

# Web Audio & Applications

Ircam - 4 Juin 2015

Samuel Goldszmidt (APM/CRI) & Norbert Schnell (ISMM)

???

SG : Développeur Web

NS : Chercheur, Designer son et interaction

---

# Plan

1. Ircam → Web Audio
2. Web Audio technologies, standards et faits
3. Ircam Web Audio librairies et framework
4. Prochaines étapes et perspectives à venir

---

layout: true

# Ircam → Web Audio

---

---

## Web Audio ?

* HTML5 Media Element &amp;lt;audio&amp;gt; &amp;lt;video&amp;gt; - <small>CRI</small>
* Web Audio pour la musicographie et la musicologie - <small> APM</small>
* Objets connectés d'intéraction musicale - <small>ISMM</small>

<img src="./media/img/medias.ircam.fr.png" height="100px" style="float:left;"/>
<img src="./media/img/just.png" height="100px" style="float:left;"/>
<img src="./media/img/pocket.png" height="100px" style="float:left;"/>
<img src="./media/img/cera.png" height="100px" />


2 projets ANR en cours :
* [WAVE](http://wave.ircam.fr) (Web Audio Visualisation/Edition) 2012/2015
* [CoSiMa](http://cosima.ircam.fr) (Collaborative Situated Media, N. Schnell) 2013/2016

Organisation de la 1<sup>ère</sup> Web Audio Conference "WAC" Janvier 2015 avec Mozilla et l'Audio Working Group du W3C

???

Musicologie, musicographie numérique

Développement d'outils d'écoute actives (projet Ecoutes signées)

Comment les compositeurs, interprètes ... écoutent.

Exemple prototype communication Flash <=> Max

---

* "Web Audio" véritablement émergent pour des applications exigentes
* Accessible sur (quasi) tous les périphériques connectés de dernière génération

#### Pourquoi transférer des savoirs, des technologies, des algorithmes de MAO et d'interactions de l'Ircam vers la plateforme Web ?

* 3 000 000 000 d'utilisateurs - s'adresser au plus grand nombre
* *Browser As An OS*, pas d'installation - "TTM" minimal
* Plateforme partagée, multi-utilisateurs, temps réel et multimédia
* Nouveaux champs d'expérimentation et de recherche


???

*Les* technologies Web Audio sont disponibles et accessible à tous

ex. Word → Google Doc

et retours utilisateurs

---

## *De l'importance* des standards

Web Audio n'est possible à grande échelle que si :
* les navigateurs respectent les standards
* les standards sont pertinents (répondent à des usages)

Jusqu'à présent :

* Adaptation à l'écosystème web - open source et [GitHub Ircam](https://github.com/Ircam-RnD)
* Utilisation et dissémination des standards (W3C track Web Audio WWW2015) - W3C
* Participation à l'émergence d'une communauté Web Audio (WAC2015, WAC2016) - Mozilla, W3C, Google

???

W3C, IETF (protocole), ECMA (js)

"Pas d'installation" si :
* Navigateur installé
* Navigateur respecte les standards

=> Aller au plus près des standards (utilisation Web Audio API et organisation WAC). Rencontré plusieurs fois W3C, les personnes qui implémentent les standard (keynotes WAC)

Continuer utiliser et faire les standards (proposition de cas d'usages musicaux concrêts, ex. hier accepté le progress callback, modification du noeud biquad filter en cours ...)

Et si une "killer app" utilise un standard mal implémenté dans un navigateur => compétition entre constructeur de navigateur => implémentation

---

layout: true

# Web Audio technologies, standards et faits

---

---

## Histoire courte

~ 1995 : &amp;lt;bgsound&amp;gt; &amp;lt;embed&amp;gt; &amp;lt;object&amp;gt; &amp;lt;applet&amp;gt;
<br>
~ 1997 : Flash - player et Flash Media Server
<br>
~ 2008 : HTML5 &amp;lt;audio&amp;gt;
<br>
~ 2010 : Web Audio API, WebRTC (getUserMedia), WebSocket ...


???

* 1995 : &amp;lt;blink&amp;gt; midifile et protocoles propriétaires (RTP, RTSP)
* 1997 : côté client avec le plugin et côté serveur avec FMS et octobre 2008 Sound API dans Flash pour faire du traitement audio, minimal
* 2008 : HTML5 &amp;lt;audio&amp;gt;
* 2010 : HTTP streaming (DASH), WebRTC et WebSockets pour l'échange de données, Wep Audio API (première implémentation Chrome 2011). Web Audio API disponible sur Edge

20 ans après la première possibilité de lire un son dans le navigateur, les standards proposent un ensemble de fonctionnalités primitives pour créer des applications audio temps réel ou non sur le web. Même s'il y a encore des choses qui manquent et à améliorer.


---

## Web Audio API (1/3)

<img src="./media/img/webaudio-js.png" height="250px" style="float:left;margin-right:40px" />

* API JavaScript de haut-niveau permettant le traitement et la synthèse audio dans des applications web
* Paradigme : un graphe de routage du signal audio construit à l'aide d'`AudioNodes`.
* L'implémentation sous-jacente est en C/C++ mais le traitement directement en JavaScript est possible.


???

La balise audio (HTML5) permet le streaming et le playback audio simple dans les navigateurs récents sans avoir recours à Flash ou QuickTime.

Mais ce n'est pas suffisant pour les jeux, les applications audio interactives (séquenceurs audios, plugin d'effets et synthétiseurs)

... et tout ce que l'on peut imaginer comme application musicale dans un environment massivement multimédia, multi-utilisateurs et collaboratif !

---
## Web Audio API (2/3)

<img src="./media/img/webaudio-js.png" height="250px" style="float:left;margin-right:40px" />
* Lecture à l'échantillon sonore près
* Intégration avec &amp;lt;audio&amp;gt;, &amp;lt;video&amp;gt;, getUserMedia et WebRTC
* Spatialisation du son
* Moteur de convolution
* Filtres, *Waveshaper*, Oscillateurs
* Paramètres audios `AudioParam`
* Analyses dans les domaines temporel et fréquenciel

???

Synthèse par distorsion non linéaire

* Analyses dans les domaines temporel et fréquenciel *pour la visualisation seulement à l'heure actuelle*

---

## Web Audio API (3/3)

* `AudioContext`
* `AudioNode`
  * Sortie uniquement : `AudioBufferSourceNode`, `MediaElementAudioSourceNode`, `MediaStreamAudioSourceNode`, `OscillatorNode`
  * Entrée uniquement : `AudioDestinationNode`, `MediaStreamAudioDestinationNode`
  * Entrée & Sortie : `GainNode`, `DelayNode`, `BiquadFilterNode`, `DynamicsCompressorNode`, `WaveShaperNode`, `PannerNode`, `StereoPannerNode`, `ConvolverNode`, `AnalyserNode`, `ChannelSplitterNode`, `ChannelMergerNode`, `ScriptProcessorNode` <small>(obsolète)</small>, `AudioWorkerNode`
* `AudioParam`


???
AudioContext contient
* le graph du signal audio <small>(connections entre AudioNodes)</small>
* les factory pour créer des AudioNodes <small>(GainNode, PannerNode ...)</small>
* la destination <small>(la sortie audio)</small>
* le currentTime <small>(utile pour ordonnancer les évènements)</small>
* le decodeAudioData

AudioParam
* contrôle un aspect individuel du fonctionnement d'un AudioNode, comme le volume
* peut être a-rate <small>valeur pour chaque échantillon d'un block</small> ou k-rate <small>valeur utilisée pour l'ensemble du block</small>
* Méthodes: setValueAtTime(), linearRampToValueAtTime(), exponentialRampToValueAtTime(), setTargetAtTime(), setValueCurveAtTime()
* peut être connecté à un AudioNode: AudioNode.connect(AudioParam) <small>(en plus de AudioNode.connect(AudioNode))</small>, usage type les LFO


---


## Exemples d'applications

[<img src="./media/img/vocoder.png" height="220px" style="float:left;margin-right:10px" />](https://webaudiodemos.appspot.com/Vocoder/index.html)

[<img src="./media/img/synth.png" height="220px" />](http://webaudiodemos.appspot.com/midi-synth/index.html)

* Vocoder : getUserMedia + Web Audio API
* Synthétiseur : Web Midi API + Web Audio API
---


## WAC et *sujets sensibles*


* Comment porter du code existant vers la Web Audio API ? Flash, CSound, PNaCl, Emscripten
* Champs habituels de la recherche informatique musicale en JavaScript natif : ex. *Music Information Retrieval* - librairie Meyda
* Outils pour les dévelopeurs : ex. Web Audio dans la console de Firefox
* Audio UI : waves.ui, P5.js, Repovizz
* Moteurs audio, framework : waves.audio, Tones.JS
* Synthèse sonore et traitement (binaural)
* Applications : MT5 (séquenceur audio), Tanguy (synthétiseur), Telemeta, Noteflight, Hyperaudio, Lissajous ...


???

WAC was the first international conference dedicated to web audio technologies and applications.
The conference gathered web R&D developers, audio processing scientists, application designers and people involved in web standards.
The conference addressed research, development, design and standards concerned with emerging audio-related web technologies such as Web Audio API, Web RTC, WebSockets and JavaScript.


Web Audio Papers
Web Audio Talks
Web Audio Demo/Poster
Web Audio Gigs
Plenary session of the W3C Audio Working Group

---

## *Web Audio API vs. Native: Closing the Gap*

Paul Adenot (Mozilla) - Keynote WAC2015

* Le code natif a de bonnes **performances** et est **flexible**
* Le code écrit avec la Web Audio API se **distribue facilement**, est **sûr** et est très **facile**.

Propositions simples pour améliorer les performances Web Audio API :


* AudioWorker
* asm.js
* SIMD.js


???
browserify
grâce au bac à sable de la plateforme web

* AudioWorker : plus de frontière de thread à traverser comme les traitements audio et JavaScript seront sur le même thread
* asm.js, langage intermédiaire constituant un sous-ensemble du langage Javascript, qui ne produit pas d'objet temporaire à passer au ramasse miette.
* SIMD.js SIMD (Single Instruction Multiple Data) qui permet de faire des opérations sur des éléments multiples ensemble, et qui a des répercutions interessantes en terme de performance sur le traitement de buffer audio.

* Pour les mauvaises nouvelles (denormals, lock-free/wait-free, context-switch), [regardez la vidéo](http://medias.ircam.fr/x2af2f6).

---

## Standards W3C liés au Web Audio (1/2)

Audio Working Group
* Web Audio API <small>Tous navigateurs sauf Android Browser</small>
* Web Midi API <small>Chrome, Opera</small>

Hors working group
* WebSocket API <small>Tous navigateurs</small>

Web Real-Time Communications Working Group
* WebRTC 1.0: Real-time Communication Between Browsers <small>Firefox, Chrome, Android Browser, Chrome for Android</small>
* Media Capture and Streams (getUserMedia) <small>Edge, Firefox, Chrome, Opera, Android browser, Chrome for android</small>
* MediaStream Recording
* Audio output devices API
* ...

---

## Standards W3C liés au Web Audio (2/2)

Geolocation Working Group
* Geolocation API Specification (puis Geolocation API Level 2) <small>Tous navigateurs</small>
* DeviceOrientation Event Specification (Device Orientation et Device Motion API) <small>Tous navigateurs sauf Safari</small>

Second Screen Presentation Working Group
* Presentation API <small>Aucun navigateur</small>

---

layout: true

# Ircam Web Audio librairies et framework

---

---

## waves.js

Une librairie pour faciliter le développement d'applications Web Audio (projet WAVE)


* UI : Affichage et édition de données temporelles
* Audio : Moteur audio et ordonnanceur
* LFO : Chaîne de traitement audio

Documentation: [wavesjs.github.io](http://wavesjs.github.io)

???

Aborde les questions que la Web Audio API et d'autres standards ne résolvent pas.

---

## waves.js - UI

Affichage et édition de données temporelles : `Waveform`, `Markers`, `Segments`, `BreakPointFunctions` (BPF) au sein d'une `Timeline` avec des possiblités de `Zoom`/`Move` et ajout de `Cursor`

<img src="./media/img/waves.ui.png" height="140px" />

[waves.js UI](http://wavesjs.github.io/examples/01-ui-what-you-can-do-with-it.html)

???

ex. HTML5 input type, pas d'affichage de waveform
Avec HTML5, de nouvelles API et de nouveau type d'input (color, range, number)
Mais rien pour pour waveform et les timeline.

Modulaire, commandé par les données, SVG

---

## waves.js - Audio

* Moteurs, jouent l'audio : `PlayerEngine`, `GranularEngine`, `SegmentEngine`
* Maîtres, contrôlent les moteurs :
  * `PlayControl`, *start - pause - stop - seek - speed ...* pour contrôler la lecture des moteurs audio
  * `Scheduler`, ordonnance/commande la lecture de moteurs audio
  * `Transport`, synchronise/coordonne la lecture de moteurs audio

<img src="./media/img/waves.audio.png" height="140px" />

[waves.js Audio](http://wavesjs.github.io/audio/)

???

ex. Web Audio API : l'ordonnancement des évènements pour des applications
 audio interactives (notamment la lecture audio) peut être très complexe à gérer.

Moteurs : créent un buffer et le jouent

1. Granular
2. Segment
3. Transport

---

## waves.js - LFO <small>(en développement)</small>

<img src="./media/img/lfo.png" height=140px/>

LFO permet :
* d'analyser et manipuler les données à travers une chaîne de traitement uniforme
* d'encapsuler les algorithmes usuels avec une interface unifiée qui peut être partagée et réutilisée

Applications : analyse/descripteurs/segmentation audio, analyse de données de captation de mouvement

???
LFO est une librairie qui propose une API qui formalise le traitement et l'analyse arbitraire de flux de données (ex. données audio, vidéo, capteurs). En normalisant le format des stream dans son entrée et sa sortie,

---

<img src="./media/img/leroux.mvt4.png" width=80%/>

["Blocs Gigognes", analyse du 4e mouvement de *Voi(rex)* de P. Leroux](http://wave.ircam.fr/demo/projects/blocs-gigognes/)

---

<img src="./media/img/webern.opus27.png" width=90%/>

[Analyse *Opus 27* de Anton Webern par E. Ducreux](http://wave.ircam.fr/demo/projects/webern-opus-27/)
---

<img src="./media/img/luna-park.aperghis.png" width=100%/>

[Analyse *Luna Park* de Georges Aperghis par J.-F Trubert](http://wave.ircam.fr/demo/luna-park/)

---

<img src="./media/img/bachotheque.png" width=90%/>


["Bachothèque" J.-S Bach](http://wave.ircam.fr/demo/projects/bachotheque/)

---

## Collective-Soundworks
Un framework pour la création d'applications audio et multimédia collaboratives (projet CoSiMa)

[Soundworks](http://github.com/collective-soundworks/soundworks)


---

layout: true

# Prochaines étapes et perspectives à venir

---

---

## Tranferts de technologies - savoirs - compétences
* Réécrire dans les langages du Web
* Utiliser des technologies comme Emscripten (LLVM → JavaScript avec asm.js)

???
Réinventer - ou réécrire

---

## Open source <strike>vs.</strike> et Open data

Mettre à disposition des résultats d'algorithmes issus de la recherche ...
<br>
... sans mettre à disposition le code source ...
<br>
... et en bénéficiant de serveurs de calculs adaptés.

---

## Technologies


* W3C Standards :
  * Web Audio API : AudioWorker
  * Performance timing, synchronisation d'appareils connectés
  * Latences des appareils
  * Implémentations des standards : Web RTC iOS, ✓ Edge Web Audio API
* JavaScript :
  * ES2015 (class, modules, promises, generator etc.)
  * ES7 (async, await)



## Human-computer interaction, and the digital arts

???
ECrire du "code non bloquant comme du code bloquant"


    </textarea>
    <script src="./static/js/remark-latest.min.js">
    </script>
    <script>
      var slideshow = remark.create();
    </script>
  </body>
</html>
